{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "from model import GeneratorModule\n",
    "from scripts.datasetscripts import parse_concatenated_json, format_augmented_goal\n",
    "import torch\n",
    "import ast"
   ],
   "id": "67fa5e9be8e3c7b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ckpt_path = \"/Users/artem.semidetnov/Desktop/MetaGenerator_v1/.cadence/cache/id2f7519f525f7457f899938c06c46d14c/45769/outputs/lightning_logs/version_6/checkpoints/last.ckpt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GeneratorModule.load_from_checkpoint(ckpt_path, strict=False)\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "id": "65c22878b0aa7b55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_path = '/Users/artem.semidetnov/Desktop/MetaGenerator_v1/datasets/dataset_small_with_addons/test.json'\n",
    "\n",
    "test_prompts = parse_concatenated_json(test_path)\n",
    "def test_model(prompt_index : int):\n",
    "    prompt = test_prompts[prompt_index]\n",
    "\n",
    "    prompt_ = format_augmented_goal(\n",
    "            prompt[\"Expected type\"],\n",
    "            prompt[\"Premises\"],\n",
    "            512,\n",
    "            0.0,\n",
    "        )\n",
    "    prompts = [prompt_]\n",
    "    enc = model.tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=getattr(model.hparams, \"max_inp_seq_len\", 512),\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generator.generate(\n",
    "            input_ids=enc[\"input_ids\"],\n",
    "            attention_mask=enc[\"attention_mask\"],\n",
    "            num_beams=4,#getattr(model.hparams, \"num_beams\", 4),\n",
    "            num_return_sequences=4,\n",
    "            length_penalty=getattr(model.hparams, \"length_penalty\", 0.0),\n",
    "            max_length=getattr(model.hparams, \"max_oup_seq_len\", 128),\n",
    "        )\n",
    "\n",
    "    preds = model.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return preds, prompt['Expression']"
   ],
   "id": "45ecb0b350634b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# len_test = len(parse_concatenated_json(test_path)) #759",
   "id": "9892aba8b0ed681b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('testoutputs', 'w') as f:\n",
    "    for i in range(700):\n",
    "        f.write(str(test_model(i)) + \"\\n\")"
   ],
   "id": "77355a4ac992b44e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T16:48:54.868261Z",
     "start_time": "2026-01-17T16:48:53.691682Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "eea09329afc97b51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: tree-sitter-python in /Users/artem.semidetnov/Library/Python/3.9/lib/python/site-packages (0.23.6)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001B[0m\r\n",
      "Failed to parse line 1: an integer is required\n",
      "Failed to parse line 2: an integer is required\n",
      "Failed to parse line 3: an integer is required\n",
      "Failed to parse line 4: an integer is required\n",
      "Failed to parse line 5: an integer is required\n",
      "2 6\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T20:53:25.351629Z",
     "start_time": "2026-01-17T20:53:25.290180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip3 install tree-sitter-python\n",
    "cnt_total = 0\n",
    "cnt_correct = 0\n",
    "\n",
    "import ast\n",
    "import os\n",
    "from codebleu import bleu, weighted_ngram_match\n",
    "import codebleu\n",
    "\n",
    "# Robust CodeBLEU calculation to handle tree-sitter failures\n",
    "def calc_codebleu_robust(references, predictions, lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "    alpha, beta, gamma, theta = weights\n",
    "\n",
    "    # Preprocess\n",
    "    references = [[x.strip() for x in ref] if isinstance(ref, list) else [ref.strip()] for ref in references]\n",
    "    hypothesis = [x.strip() for x in predictions]\n",
    "\n",
    "    # Tokenizer\n",
    "    def tokenizer(s):\n",
    "        return s.split()\n",
    "\n",
    "    tokenized_hyps = [tokenizer(x) for x in hypothesis]\n",
    "    tokenized_refs = [[tokenizer(x) for x in reference] for reference in references]\n",
    "\n",
    "    # 1. Ngram Match (BLEU)\n",
    "    ngram_match_score = bleu.corpus_bleu(tokenized_refs, tokenized_hyps)\n",
    "\n",
    "    # 2. Weighted Ngram Match\n",
    "    weighted_ngram_match_score = 0.0\n",
    "    try:\n",
    "        package_dir = os.path.dirname(codebleu.__file__)\n",
    "        keywords_path = os.path.join(package_dir, \"keywords\", f\"{lang}.txt\")\n",
    "        if os.path.exists(keywords_path):\n",
    "            with open(keywords_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                keywords = [x.strip() for x in f.readlines()]\n",
    "            \n",
    "            def make_weights(reference_tokens, key_word_list):\n",
    "                return {token: 1 if token in key_word_list else 0.2 for token in reference_tokens}\n",
    "\n",
    "            tokenized_refs_with_weights = [\n",
    "                [[reference_tokens, make_weights(reference_tokens, keywords)] for reference_tokens in reference]\n",
    "                for reference in tokenized_refs\n",
    "            ]\n",
    "            weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights, tokenized_hyps)\n",
    "        else:\n",
    "             weighted_ngram_match_score = ngram_match_score\n",
    "    except Exception:\n",
    "        weighted_ngram_match_score = ngram_match_score\n",
    "\n",
    "    # 3. Syntax & Dataflow (Try-Except)\n",
    "    syntax_match_score = 0.0\n",
    "    dataflow_match_score = 0.0\n",
    "    \n",
    "    failed_tree_sitter = False\n",
    "    if gamma > 0 or theta > 0:\n",
    "        try:\n",
    "            from codebleu import syntax_match, dataflow_match, utils\n",
    "            tree_sitter_language = utils.get_tree_sitter_language(lang)\n",
    "            \n",
    "            if gamma > 0:\n",
    "                syntax_match_score = syntax_match.corpus_syntax_match(\n",
    "                    references, hypothesis, lang, tree_sitter_language=tree_sitter_language\n",
    "                )\n",
    "            if theta > 0:\n",
    "                dataflow_match_score = dataflow_match.corpus_dataflow_match(\n",
    "                    references, hypothesis, lang, tree_sitter_language=tree_sitter_language\n",
    "                )\n",
    "        except Exception:\n",
    "            # print(f\"Tree-sitter failed\")\n",
    "            failed_tree_sitter = True\n",
    "\n",
    "    if failed_tree_sitter:\n",
    "        # Renormalize weights: distribute gamma+theta to alpha+beta\n",
    "        total_valid = alpha + beta\n",
    "        if total_valid > 0:\n",
    "            scale = (alpha + beta + gamma + theta) / total_valid\n",
    "            alpha *= scale\n",
    "            beta *= scale\n",
    "        gamma = 0\n",
    "        theta = 0\n",
    "\n",
    "    score = (\n",
    "        alpha * ngram_match_score\n",
    "        + beta * weighted_ngram_match_score\n",
    "        + gamma * syntax_match_score\n",
    "        + theta * (dataflow_match_score or 1.0)\n",
    "    )\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Safe CodeBLEU scorer with progressive fallbacks\n",
    "def score_codebleu_safe(pred: str, correct: str, lang: str = \"python\"):\n",
    "    if not isinstance(pred, str) or not isinstance(correct, str):\n",
    "        return None, \"failed\"\n",
    "    pred_s = pred.replace(\"\\x00\", \"\").strip()\n",
    "    corr_s = correct.replace(\"\\x00\", \"\").strip()\n",
    "    if not pred_s or not corr_s:\n",
    "        return None, \"empty\"\n",
    "\n",
    "    try:\n",
    "        score = calc_codebleu_robust([corr_s], [pred_s], lang=lang, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        score = max(0.0, min(1.0, score))\n",
    "        return score, \"robust_custom\"\n",
    "    except Exception as e:\n",
    "        return None, \"failed\"\n",
    "\n",
    "# For each line in testoutputs: compute the minimal CodeBLEU distance (1 - CodeBLEU score)\n",
    "min_distances = []\n",
    "max_scores = []  # 1 - min_distance (for convenience)\n",
    "components_used = []  # Which components succeeded for the best score\n",
    "\n",
    "with open(\"testoutputs\", \"r\") as f:\n",
    "    for idx, line in enumerate(f, start=1):\n",
    "        cnt_total += 1\n",
    "\n",
    "        # Parse a tuple: (preds: list[str], correct: str)\n",
    "        try:\n",
    "            parsed = ast.literal_eval(line)\n",
    "        except Exception as e:\n",
    "            print(f\"Parse error on line {idx}: {e.__class__.__name__}: {e}\")\n",
    "            min_distances.append(None)\n",
    "            max_scores.append(None)\n",
    "            continue\n",
    "\n",
    "        if not (isinstance(parsed, tuple) and len(parsed) == 2):\n",
    "            print(f\"Parse error on line {idx}: expected tuple (preds, correct), got {type(parsed)}\")\n",
    "            min_distances.append(None)\n",
    "            max_scores.append(None)\n",
    "            continue\n",
    "\n",
    "        preds, correct = parsed\n",
    "        if not isinstance(preds, list) or not all(isinstance(p, str) for p in preds) or not isinstance(correct, str):\n",
    "            print(f\"Parse error on line {idx}: wrong types. preds type={type(preds)}, correct type={type(correct)}\")\n",
    "            min_distances.append(None)\n",
    "            max_scores.append(None)\n",
    "            continue\n",
    "\n",
    "        if correct in preds:\n",
    "            cnt_correct += 1\n",
    "\n",
    "        # Compute CodeBLEU score for each prediction; convert to distance = 1 - score\n",
    "        best_score = None\n",
    "        best_comp = \"\"\n",
    "        for pred in preds:\n",
    "            # Skip empty strings to avoid tokenizer issues\n",
    "            if not pred.strip() or not correct.strip():\n",
    "                continue\n",
    "            score, label = score_codebleu_safe(pred, correct, lang=\"python\")\n",
    "            if score is None:\n",
    "                # If CodeBLEU still fails on this pair, skip it and try others\n",
    "                # Note: We keep the concise line-level message volume low.\n",
    "                # Uncomment for detailed diagnostics:\n",
    "                # print(f\"Scoring error on line {idx}: fallback failed for a pair\")\n",
    "                continue\n",
    "            if best_score is None or score > best_score:\n",
    "                best_score = score\n",
    "                best_comp = label\n",
    "\n",
    "        if best_score is None:\n",
    "            # No valid score could be computed for this line\n",
    "            min_distances.append(None)\n",
    "            max_scores.append(None)\n",
    "            components_used.append(\"\")\n",
    "        else:\n",
    "            max_scores.append(best_score)\n",
    "            min_distances.append(1.0 - best_score)\n",
    "            components_used.append(best_comp)\n",
    "\n",
    "# Save minimal distances (and corresponding max scores) to a results file for later analysis\n",
    "out_path = \"testoutputs_min_codebleu.csv\"\n",
    "with open(out_path, \"w\") as out:\n",
    "    out.write(\"line_index,min_codebleu_distance,max_codebleu_score,components_used\\n\")\n",
    "    for i, (d, s, c) in enumerate(zip(min_distances, max_scores, components_used), start=1):\n",
    "        out.write(f\"{i},{'' if d is None else d},{'' if s is None else s},{c}\\n\")\n",
    "\n",
    "print(f\"Exact-match count: {cnt_correct} out of {cnt_total}\")\n",
    "computed = sum(1 for d in min_distances if d is not None)\n",
    "print(f\"Computed CodeBLEU for {computed} / {cnt_total} lines. Results saved to {out_path}\")\n",
    "\n"
   ],
   "id": "3876fa1ef4ad305",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact-match count: 234 out of 700\n",
      "Computed CodeBLEU for 0 / 700 lines. Results saved to testoutputs_min_codebleu.csv\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T21:14:11.831923Z",
     "start_time": "2026-01-17T21:14:11.814867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnt_total = 0\n",
    "cnt_correct = 0\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "import ast  # ensure this cell is self-contained even if earlier imports weren't run\n",
    "\n",
    "# For each line in testoutputs: compute the minimal CodeBLEU distance (1 - CodeBLEU score)\n",
    "min_distances = []\n",
    "max_scores = []  # 1 - min_distance (for convenience)\n",
    "\n",
    "with open(\"testoutputs\", \"r\") as f:\n",
    "    for idx, line in enumerate(f, start=1):\n",
    "        cnt_total += 1\n",
    "        if cnt_total > 50:\n",
    "            break\n",
    "        parsed = ast.literal_eval(line)\n",
    "\n",
    "        preds, correct = parsed\n",
    "\n",
    "        # print(list(map(lambda x : x.split(\" \")[0], preds)))\n",
    "        # print(correct.split(\" \")[0])\n",
    "\n",
    "        if correct.split(\" \")[0] in list(map(lambda x : x.split(\" \")[0], preds)) and not(correct in preds):\n",
    "            print(correct.split(\" \")[0], \" | \", correct)\n",
    "            cnt_correct += 1\n",
    "cnt_correct"
   ],
   "id": "1230be13984867d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replicate  |  replicate s (zro )\n",
      "<_*-left  |  <_*-left y<z z>0 x>0\n",
      "<=<_<=  |  <=<_<= x'<=<V' idp\n",
      "\\lam  |  \\lam _ => {?}\n",
      "p  |  p Uy\n",
      "inP  |  inP (1, idp, idp)\n",
      "toZero  |  toZero {R} {g (l !! j).2} {conj a (f (l !! j).2)} p\n",
      "tupleBeta  |  tupleBeta {P1}\n",
      "\\lam  |  \\lam d => {?}\n",
      "rewrite  |  rewrite (prod-id-right ) {?}\n",
      "\\lam  |  \\lam p0 {p1} {p2} x*y=x*z => {?}\n",
      "rewrite  |  rewrite (hinv_f {associator-iso }) {?}\n",
      "\\lam  |  \\lam p0 p1 => {?}\n",
      "limUnique  |  limUnique {?}\n",
      "cauchy-subset  |  cauchy-subset p {?}\n",
      "<=-uniform  |  <=-uniform u {?} d\n",
      "func-cont  |  func-cont  OBall-open\n",
      "<=<-left  |  <=<-left {?} p\n",
      "\\lam  |  \\lam c => {?}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "947297fe2943cee9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
