{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-12T11:55:36.473255Z",
     "start_time": "2025-12-12T11:55:33.324207Z"
    }
   },
   "source": [
    "from model import GeneratorModule\n",
    "import torch"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artem.semidetnov/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:05:31.616432Z",
     "start_time": "2025-12-12T13:05:26.920250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ckpt_path = \"/Users/artem.semidetnov/Desktop/MetaGenerator_v1/.cadence/cache/id2f7519f525f7457f899938c06c46d14c/45112/outputs/lightning_logs/version_5/checkpoints/last.ckpt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GeneratorModule.load_from_checkpoint(ckpt_path, strict=False)\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "id": "b1e5c95f8f5cc2ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorModule(\n",
       "  (generator): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(384, 1472)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(384, 1472)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 6)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(384, 1472)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 6)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-3): 3 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1472, out_features=384, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:15:12.835607Z",
     "start_time": "2025-12-12T13:15:12.830834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_prompt = r'''{\"Context\": [\"l : GroupTerm V\", \"r : GroupTerm V\", \"this : NatData\", \"gl : \\\\Sigma (get-leaf (l :* r) 1 = l) (get-leaf (l :* r) 2 = r)\"], \"Expected type\": \"interpret l * interpret r = interpret l * interpret (get-leaf (l :* r) 2)\", \"Expression\": \"rewrite gl.2 idp\", \"Premises\": [\"\\\\func get-leaf \\\\hlevels  {this3 : NatData} (_ : GroupTerm V) (_ : Nat) : GroupTerm V \\n  | {this}, l :* r, ind => \\\\case dec<_<= {NatSemiring} ind (suc (count-leaves l)) \\\\with {\\n    | inl _x => get-leaf l ind\\n    | o => get-leaf r (ind -' count-leaves l)\\n  }\\n  | {this}, :inv (var x), ind => \\\\case ind \\\\with {\\n    | suc 0 => :inv {V} (var {V} x)\\n    | n => :ide {Nat}\\n  }\\n  | {this}, :inv g, ind => :ide {Nat}\\n  | {this}, var x, ind => \\\\case ind \\\\with {\\n    | suc 0 => var {V} x\\n    | n => :ide {Nat}\\n  }\\n  | {this}, :ide, ind => :ide {Nat}\", \"| \\\\infixl 7 * E E : E\", \"| G : Group\", \"\\\\func transportInv {A : \\\\Type} (B : A -> \\\\Type) {a a' : A} (_ : a = a') (_ : B a') : B a \\n  | {A}, B, {a}, {a'}, idp, b => b\", \"\\\\data GroupTerm \\\\Type \\n  | var V\\n  | :ide\\n  | :inv (GroupTerm V)\\n  | \\\\infixl 6 :* (_ _ : GroupTerm V)\", \"\\\\func \\\\infix 1 = {A : \\\\Type} (a a' : A) : \\\\Type => a = a'\", \"\\\\infixl 6 :* (_ _ : GroupTerm V)\", \"\\\\func idp {A : \\\\Type} {a : A} : a = a => path (\\\\lam (_ : I) => a)\", \"\\\\data Nat \\\\plevels  \\\\hlevels  \\n  | zero\\n  | suc Nat\", \"| E : \\\\Set\", \"\\\\func interpret \\\\hlevels  {this4 : GroupData} (_ : GroupTerm V) : E {G} \\n  | {this}, var x => f x\\n  | {this}, :ide => ide {G}\\n  | {this}, :inv t => inverse {G} (interpret t)\\n  | {this}, t :* s => interpret t * interpret s\"]}'''\n",
    "prompt2 = r'''{\"Context\": [\"y : Int\", \"p : x < y\", \"x : Int\"], \"Expected type\": \"fromInt x < fromInt y\", \"Expression\": \"<-char (simplify p)\", \"Premises\": [\"\\\\instance IntRing \\\\hlevels  : OrderedCRing.Dec Int {\\n  | zro => pos 0\\n  | + => (IntRing.+)\\n  | * => (IntRing.*)\\n  | negative => negative\\n  | ide => pos 1\\n  | natCoef => pos\\n  | #0 => AddGroup.Dec.#0\\n  | decideEq => \\\\lam (x : E {\\\\this}) (y : E {\\\\this}) => (\\\\case trichotomy {\\\\this} x y \\\\with {\\n    | less x<y => no {x = y}\\n      (\\\\lam (x=y : x = y) => <-irreflexive {\\\\this} {y} (transport {E {\\\\this}} (\\\\lam (_x : E {\\\\this}) => _x < y) {x} {y} x=y x<y))\\n    | equals x=y => yes {x = y} x=y\\n    | greater y<x => no {x = y} (\\\\lam (x=y : x = y) => <-irreflexive {\\\\this} {y} (transport {E {\\\\this}} ((<) {\\\\this} y) {x} {y} x=y y<x))\\n  })\\n  | meet => meet\\n  | join => join\\n  | isPos => \\\\lam (x : E {\\\\this}) => signum x = pos 1\\n  | #0=>eitherPosOrNeg => #0=>eitherPosOrNeg\\n  | +_trichotomy => \\\\lam (x : E {\\\\this}) => (\\\\case x \\\\with {\\n    | pos 0 => equals {pos 0} {pos 0} (idp {Int} {pos 0})\\n    | pos (suc n) => greater {pos (suc n)} {pos 0} (idp {Int} {signum (pos (suc n) - pos 0)})\\n    | neg (suc n) => less {neg (suc n)} {pos 0} (idp {Int} {signum (pos 0 - neg (suc n))})\\n  })\\n} => \\\\new OrderedCRing.Dec {\\n  | zro-left => {?hidden}\\n  | zro-right => {?hidden}\\n  | +-assoc => {?hidden}\\n  | +-comm => {?hidden}\\n  | *-assoc => {?hidden}\\n  | ldistr => {?hidden}\\n  | rdistr => {?hidden}\\n  | negative-left => {?hidden}\\n  | ide-left => {?hidden}\\n  | ide-right => {?hidden}\\n  | natCoefZero => {?hidden}\\n  | natCoefSuc => {?hidden}\\n  | zro/=ide => {?hidden}\\n  | zeroProduct => {?hidden}\\n  | meet-left => {?hidden}\\n  | meet-right => {?hidden}\\n  | meet-univ => {?hidden}\\n  | join-left => {?hidden}\\n  | join-right => {?hidden}\\n  | join-univ => {?hidden}\\n  | zro/>0 => {?hidden}\\n  | positive_+ => {?hidden}\\n  | ide>zro => {?hidden}\\n  | positive_* => {?hidden}\\n  | positive=>#0 => {?hidden}\\n  | negative=>#0 => {?hidden}\\n  | *-comm => {?hidden}\\n}\", \"\\\\func transport {A : \\\\Type} (B : A -> \\\\Type) {a a' : A} (p : a = a') (b : B a) : B a' => coe (\\\\lam (i : I) => B (p @ i)) b right\", \"| ide : E\", \"\\\\instance RatField : DiscreteOrderedField Rat {\\n  | zro => rat (pos 0) 1 {?hidden} {?hidden}\\n  | + => (RatField.+)\\n  | * => (RatField.*)\\n  | negative => RatField.negative\\n  | ide => rat (pos 1) 1 {?hidden} {?hidden}\\n  | natCoef => \\\\lam (n : Nat) => rat (pos n) 1 {?hidden} {?hidden}\\n  | decideEq => \\\\lam (x : E {\\\\this}) (y : E {\\\\this}) => (\\\\case decideEq (ratNom x) (ratNom y), decideEq (ratDenom x) (ratDenom y) \\\\with {\\n    | yes p, yes q => yes {x = y} (ext {x} {y} p q)\\n    | no p, d => no {x = y} (\\\\lam (x=y : x = y) => p (pmap {Rat} {Int} ratNom {x} {y} x=y))\\n    | d, no q => no {x = y} (\\\\lam (x=y : x = y) => q (pmap {Rat} {Nat} ratDenom {x} {y} x=y))\\n  })\\n  | meet => meet\\n  | join => join\\n  | isPos => \\\\lam (x : E {\\\\this}) => isPos (ratNom x)\\n  | #0=>eitherPosOrNeg => \\\\lam {x : E {\\\\this}} (xInv : #0 {\\\\this} x) => (\\\\case x, xInv \\\\with {\\n    | rat n1 d1 d1/=0 r1, (rat n2 d2 d2/=0 r2, il, p) => #0=>eitherPosOrNeg {n1}\\n      (\\\\lam (n1=0 : n1 = zro) => (\\\\case inv {Rat} {makeRat zro (d2 Nat.* d1) (productNonZero {d2} {d1} {?hidden} {?hidden})}\\n        {makeRat' zro (d2 Nat.* d1) (productNonZero {d2} {d1} {?hidden} {?hidden})}\\n        (simp {zro} {d2 Nat.* d1} {productNonZero {d2} {d1} {?hidden} {?hidden}}) *> transport {Int}\\n        (\\\\lam (x : E) => makeRat x (d2 Nat.* d1) (productNonZero {d2} {d1} {?hidden} {?hidden}) = rat (pos 1) 1 {?hidden} {?hidden})\\n        {n2 Semigroup.* zro} {zro} (zro_*-right {n2}) (transport {Int}\\n          (\\\\lam (x : E) => makeRat (n2 IntRing.* x) (d2 Nat.* d1) (productNonZero {d2} {d1} {?hidden} {?hidden}) = rat (pos 1) 1 {?hidden} {?hidden})\\n          {n1} {zro} n1=0 il) \\\\with {}))\\n  })\\n  | finv => RatField.finv\\n  | eitherZeroOrInv => \\\\lam (x : E {\\\\this}) => (\\\\case decideEq {\\\\this} x (zro {\\\\this}) \\\\with {\\n    | yes x=0 => byLeft {x = zro {\\\\this}} {Inv x} x=0\\n    | no x/=0 => byRight {x = zro {\\\\this}} {Inv x} (rmake {x} (DiscreteField.finv {\\\\this} x) (finv-right {\\\\this} {x} x/=0))\\n  })\\n} => \\\\new DiscreteOrderedField {\\n  | zro-left => {?hidden}\\n  | zro-right => {?hidden}\\n  | +-assoc => {?hidden}\\n  | +-comm => {?hidden}\\n  | *-assoc => {?hidden}\\n  | ldistr => {?hidden}\\n  | rdistr => {?hidden}\\n  | negative-left => {?hidden}\\n  | ide-left => {?hidden}\\n  | ide-right => {?hidden}\\n  | natCoefZero => {?hidden}\\n  | natCoefSuc => {?hidden}\\n  | zeroProduct => {?hidden}\\n  | meet-left => {?hidden}\\n  | meet-right => {?hidden}\\n  | meet-univ => {?hidden}\\n  | join-left => {?hidden}\\n  | join-right => {?hidden}\\n  | join-univ => {?hidden}\\n  | zro/>0 => {?hidden}\\n  | positive_+ => {?hidden}\\n  | ide>zro => {?hidden}\\n  | positive_* => {?hidden}\\n  | *-comm => {?hidden}\\n  | finv_zro => {?hidden}\\n  | finv-right => {?hidden}\\n}\", \"\\\\lemma <-char \\\\plevels  \\\\hlevels  {q r : Rat} (_ : ratNom q * pos (ratDenom r) < ratNom r * pos (ratDenom q)) : q < r \", \"\\\\func fromInt \\\\plevels  \\\\hlevels  (x : Int) : Rat => rat x 1 {?hidden} {?hidden}\", \"pos Nat\", \"| \\\\infixl 7 * E E : E\", \"\\\\data Int \\\\plevels  \\\\hlevels  \\n  | pos Nat\\n  | neg Nat  {\\n    | 0 => pos 0\\n  }\", \"\\\\func * (_ _ : Int) : Int \\n  | pos n, pos m => pos (n * m)\\n  | pos n, neg m => neg (n * m)\\n  | neg (suc n), pos m => neg (suc n * m)\\n  | neg (suc n), neg m => pos (suc n * m)\", \"\\\\func inv {A1 : \\\\Type} {a1 a' : A1} (_ : a1 = a') : a' = a1 \\n  | {A}, {a}, {a'}, idp => idp {A} {a}\", \"\\\\func ratDenom \\\\plevels  \\\\hlevels  (_ : Rat) : Nat \\n  | rat nom d denom/=0 reduced => d\", \"\\\\func ratNom \\\\plevels  \\\\hlevels  (_ : Rat) : Int \\n  | rat n denom denom/=0 reduced => n\", \"\\\\type \\\\infix 4 < \\\\hlevels  {A : PreorderedAddGroup} (x y : A.E) : \\\\Prop => A.isPos (y - x)\", \"| ide-right {x : E} : x * ide = x\"]}'''\n",
    "\n",
    "prompt3 = r'''{\"Context\": [\"l : Array E\", \"this : CGroup\"], \"Expected type\": \"Array E l.len (\\\\lam (i : Fin l.len) => inverse (l.at i))\", \"Expression\": \"map (inverse {this}) l\", \"Premises\": [\"| len : Nat\", \"\\\\data Fin \\\\plevels  \\\\hlevels  Nat \\\\with\\n  | suc n => zero\\n  | suc n => suc (Fin n)\", \"| at (j : Fin len) : A j\", \"\\\\class DArray {\\n  | len : Nat\\n  | A (Fin len) : \\\\Type\\n  | at (j : Fin len) : A j\\n}\", \"| E : \\\\Set\", \"| inverse E : E\", \"\\\\func map {A B : \\\\Type} (f : A -> B) (as : Array A) : Array B as.len (\\\\lam (i : Fin as.len) => f (as.at i)) => \\\\new DArray\"]}'''\n",
    "prompts = [test_prompt, prompt2, prompt3]"
   ],
   "id": "45d91aa3489f0770",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:15:13.545057Z",
     "start_time": "2025-12-12T13:15:13.536443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enc = model.tokenizer(\n",
    "    prompts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=getattr(model.hparams, \"max_inp_seq_len\", 512),\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "enc = {k: v.to(device) for k, v in enc.items()}"
   ],
   "id": "c839abdc0d6eb13b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:15:17.091530Z",
     "start_time": "2025-12-12T13:15:13.933859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generator.generate(\n",
    "        input_ids=enc[\"input_ids\"],\n",
    "        attention_mask=enc[\"attention_mask\"],\n",
    "        num_beams=getattr(model.hparams, \"num_beams\", 4),\n",
    "        length_penalty=getattr(model.hparams, \"length_penalty\", 0.0),\n",
    "        max_length=getattr(model.hparams, \"max_oup_seq_len\", 128),\n",
    "    )\n"
   ],
   "id": "2f485a46b76fb4c1",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T13:15:17.146071Z",
     "start_time": "2025-12-12T13:15:17.108279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds = model.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for i, p in enumerate(preds):\n",
    "    print(f\"[{i}] {p}\")"
   ],
   "id": "5cc7517e9f536e83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] \\lam p0 => (\\case \\elim p0 \\with {\n",
      "  | inP (U, CU, inP (U, CU, idp) => inP (U, CU, idp)\n",
      "})\n",
      "[1] \\lam p0 => (\\case \\elim p0 \\with {\n",
      "  | inP (U, CU, inP (U, CU, idp) => inP (U, CU, idp)\n",
      "})\n",
      "[2] \\lam p0 => (\\case \\elim p0 \\with {\n",
      "  | inP (U, CU, inP (U, CU, idp) => inP (U, CU, idp)\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d82f7719d2d5bcf5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
