{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:10:12.499672Z",
     "start_time": "2025-12-11T20:10:12.496310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "import datasetscripts as pp\n",
    "from pprint import pprint"
   ],
   "id": "50873d28208dcff5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:56:19.213553Z",
     "start_time": "2025-12-10T21:56:19.045342Z"
    }
   },
   "cell_type": "code",
   "source": "!ls -la ../",
   "id": "e60eac5de7b5f1c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\r\n",
      "drwxr-xr-x   9 artem.semidetnov  staff  288 Dec 10 22:48 \u001B[1m\u001B[36m.\u001B[m\u001B[m\r\n",
      "drwx------@ 10 artem.semidetnov  staff  320 Dec 10 22:35 \u001B[1m\u001B[36m..\u001B[m\u001B[m\r\n",
      "drwxr-xr-x@  3 artem.semidetnov  staff   96 Dec 10 22:35 \u001B[1m\u001B[36m.cadence\u001B[m\u001B[m\r\n",
      "drwxr-xr-x  10 artem.semidetnov  staff  320 Dec 10 22:38 \u001B[1m\u001B[36m.git\u001B[m\u001B[m\r\n",
      "drwxr-xr-x@  4 artem.semidetnov  staff  128 Dec 10 22:35 \u001B[1m\u001B[36m.idea\u001B[m\u001B[m\r\n",
      "drwxr-xr-x@  3 artem.semidetnov  staff   96 Dec 10 22:41 \u001B[1m\u001B[36mdata\u001B[m\u001B[m\r\n",
      "-rw-r--r--@  1 artem.semidetnov  staff  519 Dec 10 22:35 main.py\r\n",
      "-rw-r--r--@  1 artem.semidetnov  staff   67 Dec 10 22:48 requirements.txt\r\n",
      "drwxr-xr-x@  4 artem.semidetnov  staff  128 Dec 10 22:56 \u001B[1m\u001B[36mscripts\u001B[m\u001B[m\r\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:54:42.742232Z",
     "start_time": "2025-12-10T21:54:41.450579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parsed arend dataOld\n",
    "arend_data = pp.parse_concatenated_json(\"/dataOld/data.json\")\n",
    "\n",
    "# Paths for the new dataset\n",
    "path_train = \"../datasets/dataset/train.json\"\n",
    "path_valid = \"../datasets/dataset/val.json\"\n",
    "path_test = \"../datasets/dataset/test.json\"\n",
    "\n",
    "# Sizes for random sampling for the new dataset\n",
    "train_size = 30_000\n",
    "valid_size = 1_000\n",
    "test_size = 1_000"
   ],
   "id": "ca7d1b8c148583bb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:56:44.454368Z",
     "start_time": "2025-12-10T21:56:43.891390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "total_size = train_size + valid_size + test_size\n",
    "total_sample = random.sample(arend_data, total_size)\n",
    "train_sample, valid_sample, test_sample = random_split(total_sample, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "with open(path_train, 'w', encoding='utf-8') as f:\n",
    "    for entry in train_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "valid_sample = total_sample[train_size : train_size  + valid_size]\n",
    "with open(path_valid, 'w', encoding='utf-8') as f:\n",
    "    for entry in valid_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "test_sample = total_sample[train_size + valid_size : train_size + valid_size + test_size]\n",
    "with open(path_test, 'w', encoding='utf-8') as f:\n",
    "    for entry in test_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n"
   ],
   "id": "ed8e9591192a310e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Small one",
   "id": "23a59f6349291788"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:54:08.239413Z",
     "start_time": "2025-12-11T20:54:06.820964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parsed arend dataOld\n",
    "arend_data = pp.parse_concatenated_json(\"/dataOld/data.json\")\n",
    "\n",
    "# Paths for the new dataset\n",
    "path_train = \"../datasets/dataset_very_small/train.json\"\n",
    "path_valid = \"../datasets/dataset_very_small/val.json\"\n",
    "path_test = \"../datasets/dataset_very_small/test.json\"\n",
    "\n",
    "# Sizes for random sampling for the new dataset\n",
    "train_size = 1_000\n",
    "valid_size = 100\n",
    "test_size = 100"
   ],
   "id": "85e76d904ae258e0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:54:13.507415Z",
     "start_time": "2025-12-11T20:54:13.456202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "total_size = train_size + valid_size + test_size\n",
    "total_sample = random.sample(arend_data, total_size)\n",
    "train_sample, valid_sample, test_sample = random_split(total_sample, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "with open(path_train, 'w', encoding='utf-8') as f:\n",
    "    for entry in train_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "valid_sample = total_sample[train_size : train_size  + valid_size]\n",
    "with open(path_valid, 'w', encoding='utf-8') as f:\n",
    "    for entry in valid_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "test_sample = total_sample[train_size + valid_size : train_size + valid_size + test_size]\n",
    "with open(path_test, 'w', encoding='utf-8') as f:\n",
    "    for entry in test_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ],
   "id": "2e636772dee0783f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53904257ec5b8b3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset with smaller premises",
   "id": "689dc253604b9203"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T12:32:47.761234Z",
     "start_time": "2025-12-17T12:32:47.755197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "import datasetscripts as pp\n",
    "from pprint import pprint"
   ],
   "id": "b7fc76f0b9462ba7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T12:33:48.060226Z",
     "start_time": "2025-12-17T12:33:47.526170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parsed arend dataOld\n",
    "arend_data = pp.parse_concatenated_json(\"../datasets/data1/data.json\")\n",
    "\n",
    "# Paths for the new dataset\n",
    "path_train = \"../datasets/dataset1/train.json\"\n",
    "path_valid = \"../datasets/dataset1/val.json\"\n",
    "path_test = \"../datasets/dataset1/test.json\"\n",
    "\n",
    "# Sizes for random sampling for the new dataset\n",
    "train_size = 35_000\n",
    "valid_size = 1_000\n",
    "test_size = 1_000"
   ],
   "id": "74d6b556cc24408d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T12:34:05.765345Z",
     "start_time": "2025-12-17T12:34:05.474679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "total_size = train_size + valid_size + test_size\n",
    "total_sample = random.sample(arend_data, total_size)\n",
    "train_sample, valid_sample, test_sample = random_split(total_sample, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "with open(path_train, 'w', encoding='utf-8') as f:\n",
    "    for entry in train_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "valid_sample = total_sample[train_size : train_size  + valid_size]\n",
    "with open(path_valid, 'w', encoding='utf-8') as f:\n",
    "    for entry in valid_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "test_sample = total_sample[train_size + valid_size : train_size + valid_size + test_size]\n",
    "with open(path_test, 'w', encoding='utf-8') as f:\n",
    "    for entry in test_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ],
   "id": "fd6c0e7a78b59d13",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## smaller dataset",
   "id": "17e2718695ca02b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:51:22.555494Z",
     "start_time": "2025-12-17T15:51:21.875638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parsed arend dataOld\n",
    "arend_data = pp.parse_concatenated_json(\"../datasets/data1/data.json\")\n",
    "\n",
    "# Paths for the new dataset\n",
    "path_train = \"../datasets/dataset_small1/train.json\"\n",
    "path_valid = \"../datasets/dataset_small1/val.json\"\n",
    "path_test = \"../datasets/dataset_small1/test.json\"\n",
    "\n",
    "# Sizes for random sampling for the new dataset\n",
    "train_size = 10_000\n",
    "valid_size = 1100\n",
    "test_size = 100"
   ],
   "id": "3ac41d2af69c2c82",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:51:31.971321Z",
     "start_time": "2025-12-17T15:51:31.843160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "total_size = train_size + valid_size + test_size\n",
    "total_sample = random.sample(arend_data, total_size)\n",
    "train_sample, valid_sample, test_sample = random_split(total_sample, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "with open(path_train, 'w', encoding='utf-8') as f:\n",
    "    for entry in train_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "valid_sample = total_sample[train_size : train_size  + valid_size]\n",
    "with open(path_valid, 'w', encoding='utf-8') as f:\n",
    "    for entry in valid_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "test_sample = total_sample[train_size + valid_size : train_size + valid_size + test_size]\n",
    "with open(path_test, 'w', encoding='utf-8') as f:\n",
    "    for entry in test_sample:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ],
   "id": "2875b80df3f7be8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "24f5aaf1e9c17e09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
